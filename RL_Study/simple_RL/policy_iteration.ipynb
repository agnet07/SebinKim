{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2b6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d52fde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebinkim/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbdf5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "\t\"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\tpolicy: np.array[nS]\n",
    "\t\tThe policy to evaluate. Maps states to actions.\n",
    "\ttol: float\n",
    "\t\tTerminate policy evaluation when\n",
    "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
    "\tReturns\n",
    "\t-------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\t\tThe value function of the given policy, where value_function[s] is\n",
    "\t\tthe value of state s\n",
    "\t\"\"\"\n",
    "\n",
    "\tvalue_function = np.zeros(nS)\n",
    "\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\t#print(policy)\n",
    "\twhile True:\n",
    "\t\tdelta = 0\n",
    "\t\t#for all state...\n",
    "\t\tfor each_State in range(nS) :\n",
    "\t\t\t# v <- V(s)\n",
    "\t\t\tvalue_state_x = value_function[each_State]\n",
    "\n",
    "\t\t\t# V(s) <- Bellman Equation of state action value\n",
    "\t\t\taction_value = np.zeros(nA)\n",
    "\t\t\ttmp_s_a_val = 0\n",
    "\t\t\t#print(sum_sr_2(P, value_function, each_State, policy[each_State], gamma))\n",
    "\t\t\tfor t_p, next_state, reward_t, _ in P[each_State][policy[each_State]] :\n",
    "\t\t\t\ttmp_s_a_val += t_p*(reward_t+(gamma*value_function[next_state]))\n",
    "\t\t\tvalue_function[each_State] = tmp_s_a_val\n",
    "\t\t\t#print(tmp_s_a_val)\n",
    "\t\t\t#print(action_value)\n",
    "\t\t\t# delta <- max |value_function(s) - prev_value_function(s)| < tol\n",
    "\t\t\tdelta = max(delta, abs(value_state_x-value_function[each_State]))\n",
    "\t\t\t#print(\"delta_tol = \",delta, \"  vs \", tol)\n",
    "\t\t# Terminate policy evaluation when delta < tol\n",
    "\t\tif delta < tol:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t############################\n",
    "\treturn value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83e8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.9\n",
    "tol=1e-3\n",
    "P = env.P\n",
    "nS = env.nS\n",
    "nA = env.nA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c9aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function = np.zeros(nS)\n",
    "policy = np.zeros(nS, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba22dff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_evaluation(P, nS, nA, policy, gamma, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a521a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "\t\"\"\"Given the value function from policy improve the policy.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\tvalue_from_policy: np.ndarray\n",
    "\t\tThe value calculated from the policy\n",
    "\tpolicy: np.array\n",
    "\t\tThe previous policy.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tnew_policy: np.ndarray[nS]\n",
    "\t\tAn array of integers. Each integer is the optimal action to take\n",
    "\t\tin that state according to the environment dynamics and the\n",
    "\t\tgiven value function.\n",
    "\t\"\"\"\n",
    "\n",
    "\tnew_policy = np.zeros(nS, dtype='int')\n",
    "\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "\t# For each State\n",
    "\tfor each_state in range(nS) :\n",
    "\t\t# V(s) <- Bellman Equation of state action value\n",
    "\t\tlist_of_action = []\n",
    "\t\tfor each_action in range(nA) :\n",
    "\t\t\ttmp_s_a_val = 0\n",
    "\t\t\tfor t_p, next_state, reward_t, _ in P[each_state][each_action] :\n",
    "\t\t\t\ttmp_s_a_val += t_p*(reward_t+(gamma*value_from_policy[next_state]))\n",
    "\t\t\tlist_of_action.append(tmp_s_a_val)\n",
    "\t\t# argmax of the action-value\n",
    "\t\tnew_policy[each_state] = np.argmax(list_of_action)\n",
    "\n",
    "\t############################\n",
    "\treturn new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f959ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_improvement(P, nS, nA, value_function, policy, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77abf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
    "\t\"\"\"Runs policy iteration.\n",
    "\n",
    "\tYou should call the policy_evaluation() and policy_improvement() methods to\n",
    "\timplement this method.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\ttol: float\n",
    "\t\ttol parameter used in policy_evaluation()\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\tpolicy: np.ndarray[nS]\n",
    "\t\"\"\"\n",
    "\n",
    "\tvalue_function = np.zeros(nS)\n",
    "\tpolicy = np.zeros(nS, dtype=int)\n",
    "\titer_n = 0\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\twhile True :\n",
    "\t\titer_n += 1\n",
    "\t\tvalue_function = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
    "        \n",
    "\t\told_policy = policy\n",
    "\t\tpolicy = policy_improvement(P, nS, nA, value_function, policy, gamma)\n",
    "        \n",
    "\t\tbreak_con = True\n",
    "\t\tfor each_state in range(nS) :\n",
    "\t\t\tif old_policy[each_state] != policy[each_state] :\n",
    "\t\t\t\tbreak_con = False\n",
    "        \n",
    "\t\tif break_con :\n",
    "\t\t\tbreak\n",
    "            \n",
    "\tprint(iter_n)\n",
    "\t############################\n",
    "\treturn value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e376e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.59 , 0.656, 0.729, 0.656, 0.656, 0.   , 0.81 , 0.   , 0.729,\n",
       "        0.81 , 0.9  , 0.   , 0.   , 0.9  , 1.   , 0.   ]),\n",
       " array([1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786bd30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
